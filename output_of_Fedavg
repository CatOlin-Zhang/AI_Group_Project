D:\ANACONDA\envs\AI\python.exe C:\Users\阿根达斯\PycharmProjects\AI_Group_Project\merge_and_evaluate.py
Loading IMDb test set (Proportion used: 5.0%)...
Final number of test samples: 1250

==================================================
Initializing training subset (for overfitting analysis)...
Load IMDb training dataset subset (Proportion used: 5.0%) Used for overfitting analysis...
Use 1250 training sample

Evaluating the standalone model...


--- Model 1: best_bert_model(1).bin ---
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Assessment: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]
Accuracy: 0.7352 | F1 (macro): 0.7322 | F1 (weighted): 0.7316
Confusion Matrix:
[[393 244]
 [ 87 526]]

Assessing the overfitting risk of  Model 1 ...
Assessment: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]
Assessment: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]
Model 1 Training subset: Accuracy=0.7352, F1(macro)=0.7323
Model 1 Test subset: Accuracy=0.7352, F1(macro)=0.7322
Accuracy gap (Train - Test): 0.0000
F1 gap (Train - Test): 0.0001
Model 1 generalizes well, no significant overfitting

--- Model 2: best_bert_model(2).bin ---
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Assessment: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]
Assessment:   0%|          | 0/79 [00:00<?, ?it/s]Accuracy: 0.6848 | F1 (macro): 0.6596 | F1 (weighted): 0.6614
Confusion Matrix:
[[598  39]
 [355 258]]

Assessing the overfitting risk of  Model 2 ...
Assessment: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.53it/s]
Model 2 Training subset: Accuracy=0.6528, F1(macro)=0.6210
Model 2 Test subset: Accuracy=0.6848, F1(macro)=0.6596
Accuracy gap (Train - Test): -0.0320
F1 gap (Train - Test): -0.0386
Model 2 generalizes well, no significant overfitting

--- Model 3: best_bert_model(3).bin ---
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.53it/s]
Assessment:   0%|          | 0/79 [00:00<?, ?it/s]Accuracy: 0.9448 | F1 (macro): 0.9448 | F1 (weighted): 0.9448
Confusion Matrix:
[[598  39]
 [ 30 583]]

Assessing the overfitting risk of  Model 3 ...
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.53it/s]
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.54it/s]
Model 3 Training subset: Accuracy=0.9856, F1(macro)=0.9856
Model 3 Test subset: Accuracy=0.9448, F1(macro)=0.9448
Accuracy gap (Train - Test): 0.0408
F1 gap (Train - Test): 0.0408
Notice: Model 3 has mild overfitting
Skipping best_bert_model(4).bin: file does not exist
Skipping best_bert_model(5).bin: file does not exist
Skipping best_bert_model(6).bin: file does not exist
Skipping best_bert_model(7).bin: file does not exist
Eliminate low-performance models best_bert_model(2).bin (F1=0.6596 < 0.7)

Executing weighted FedAvg (assigning weights based on F1(macro), threshold=0.7)...
Model weight allocation:
  Model 1 (best_bert_model(1).bin): F1(macro)=0.7322, Weight=0.4366
  Model 2 (best_bert_model(3).bin): F1(macro)=0.9448, Weight=0.5634
Weighted merge completed! Saved to: fedavg_merged_bert_model.bin

Evaluating the merged model...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.54it/s]
Assessment:   0%|          | 0/79 [00:00<?, ?it/s]
Final result of the merged model:
Accuracy: 0.9328 | F1 (macro): 0.9328 | F1 (weighted): 0.9328
Confusion Matrix:
[[595  42]
 [ 42 571]]

==================================================

Assessing the overfitting risk of  Combined Model ...
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.54it/s]
Assessment: 100%|██████████| 79/79 [00:51<00:00,  1.52it/s]
Combined Model Training subset: Accuracy=0.9704, F1(macro)=0.9704
Combined Model Test subset: Accuracy=0.9328, F1(macro)=0.9328
Accuracy gap (Train - Test): 0.0376
F1 gap (Train - Test): 0.0376
Notice: Combined Model has mild overfitting

Best single model: Accuracy=0.9448, F1(macro)=0.9448
Merged model: Accuracy=0.9328, F1(macro)=0.9328

进程已结束，退出代码为 0